from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json

# 1. åŠ è½½çŸ¥è¯†ç‚¹æ•°æ®
with open('clouds/point.json', 'r', encoding='utf-8') as f:
    knowledge_points = json.load(f)

# 2. å‡†å¤‡çŸ¥è¯†ç‚¹æ–‡æœ¬
knowledge_texts = [kp['name'] for kp in knowledge_points]

# 3. åŠ è½½å¥å‘é‡æ¨¡å‹
model = SentenceTransformer('moka-ai/m3e-base')

# 4. çŸ¥è¯†ç‚¹ç¼–ç ä¸ºå‘é‡
knowledge_vectors = model.encode(knowledge_texts, normalize_embeddings=True)

# 5. æ„å»º FAISS ç´¢å¼•
dim = knowledge_vectors.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(np.array(knowledge_vectors))

# 6. ç»™å®šé¢˜ç›®è¿›è¡ŒåŒ¹é…
def match_question_to_knowledge(question, top_k=5):
    question_vector = model.encode([question], normalize_embeddings=True)
    scores, indices = index.search(question_vector, top_k)
    results = []
    for idx in indices[0]:
        kp = knowledge_points[idx]
        results.append({
            "id": kp["id"],
            "name": kp["name"],
            "parentId": kp["parentId"]
        })
    return results

# 7. ç¤ºä¾‹æµ‹è¯•
with open('clouds/test.json', 'r', encoding='utf-8') as f:
    test_data = json.load(f)

total = 0
full_match = 0
partial_match = 0
result_records = []

for item in test_data:
    question = item.get("stem", "")
    true_ids = set(item.get("knowledgePointIds", []))
    true_kps = set(item.get("knowledgePointNames", []))

    if not question or not true_ids:
        continue

    matched = match_question_to_knowledge(question, top_k=2)
    predicted_ids = [m["id"] for m in matched]
    predicted_names = [m["name"] for m in matched]

    # åˆ¤æ–­åŒ¹é…ç»“æœ
    if predicted_ids[0] in true_ids:
        match_type = "full"
        full_match += 1
    elif any(pid in true_ids for pid in predicted_ids):
        match_type = "partial"
        partial_match += 1
    else:
        match_type = "none"

    total += 1

    # ä¿å­˜åŒ¹é…è®°å½•
    result_records.append({
        "stem": question,
        "knowledgePointIds": list(true_ids),
        "knowledgePointNames": list(true_kps),
        "predicted": [{"id": pid, "name": pname} for pid, pname in zip(predicted_ids, predicted_names)],
        "match": match_type
    })

# 9. è¾“å‡ºè¯„ä¼°ç»“æœ
print("\nğŸ“Š åŒ¹é…è¯„ä¼°ç»“æœï¼š")
print(f"æ€»é¢˜æ•°ï¼š{total}")
print(f"å®Œå…¨åŒ¹é…æ•°ï¼š{full_match}")
print(f"éƒ¨åˆ†åŒ¹é…æ•°ï¼ˆTop2 å‘½ä¸­ï¼‰ï¼š{partial_match}")
print(f"å®Œå…¨åŒ¹é…ç‡ï¼š{full_match / total:.2%}")
print(f"Top2 åŒ¹é…ç‡ï¼š{(full_match + partial_match) / total:.2%}")

# 10. ä¿å­˜ç»“æœ
with open('result.json', 'w', encoding='utf-8') as f:
    json.dump(result_records, f, ensure_ascii=False, indent=2)

print("\nâœ… æ¯é¢˜åŒ¹é…ç»“æœå·²ä¿å­˜è‡³ result.json")

